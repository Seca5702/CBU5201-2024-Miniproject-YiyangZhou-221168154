{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Veracity of Narrated Stories Using Audio Feature Extraction and Ensemble Machine Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaGn4ICrfqXZ"
   },
   "source": [
    "# 1 Author\n",
    "\n",
    "**Student Name**: Yiyang Zhou <br>\n",
    "**Student ID**: 221168154\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o38VQkcdKd6k"
   },
   "source": [
    "# 2 Problem Formulation\n",
    "\n",
    "The goal of this project is to build a machine learning model that predicts whether a narrated story is real or fictional based on a 2-5 minute audio recording. The formalized description of this project is shown as follows:\n",
    "\n",
    "**2.1 Audio Classification Challenge**:\n",
    "   - Processing and analyzing complex audio features\n",
    "   - Handling variations in speaking styles and emotional expressions\n",
    "   - Extracting meaningful patterns from high-dimensional acoustic data\n",
    "\n",
    "**2.2 Technical Components**:\n",
    "   - Audio signal processing and feature extraction\n",
    "   - Data augmentation for robust model training\n",
    "   - Ensemble learning with multiple classifiers:\n",
    "     * Support Vector Machines (SVM)\n",
    "     * Random Forest (RF)\n",
    "     * K-Nearest Neighbors (KNN)\n",
    "\n",
    "**2.3 Research Objectives**:\n",
    "   - Develop an accurate and robust classification system\n",
    "   - Compare and combine different machine learning approaches\n",
    "   - Achieve improved performance through ensemble methods\n",
    "\n",
    "**2.4 What's Interesting About the Project**:\n",
    "\n",
    "- **Data Set Expansion**: Despite the initial small scale of available datasets, innovative strategies are required to expand the data pool, enhancing the model's learning capabilities and generalization.\n",
    "- **Cross-Disciplinary Impact**: The project intersects multiple disciplines, from machine learning and audio processing to psychology and security, showcasing its broad applicability and potential for real-world impact.\n",
    "- **Limitation of deep learning**: Small dataset size also implies that deep learning algorithms, which generally perform well but require larger datasets, may not perform as well as classical machine learning algorithms in this project.\n",
    "\n",
    "The innovative use of ensemble learning methods helps overcome individual model limitations and provides a more reliable solution for deception detection in audio narratives.\n",
    "\n",
    "By addressing these challenges, the project aims to help people better identify the authenticity of verbal messages, which has important implications in the fields of fraud detection and prevention, psychological analysis and behavioral research, security and forensics applications, and human-computer interaction systems.\n",
    "\n",
    "This project has been uploaded to GitHub. The GitHub link for this project is [https://github.com/Seca5702/CBU5201-2024-Miniproject-YiyangZhou-221168154](https://github.com/Seca5702/CBU5201-2024-Miniproject-YiyangZhou-221168154)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPTSuaB9L2jU"
   },
   "source": [
    "# 3 Methodology\n",
    "\n",
    "**3.1 Training Task**\n",
    "\n",
    "The training task involves constructing a supervised learning pipeline to classify audio samples into \"Deceptive Story\" (0) or \"True Story\" (1). Training data, augmented through pitch shifting, noise addition, and speed alteration, is processed into standardized acoustic features such as MFCC, chroma features, and Mel spectrograms. These features are creatively used to train three base classifiers (SVM, Random Forest, and KNN) independently, with hyperparameters optimized using grid search and cross-validation. The trained classifiers are then integrated into an ensemble model using a soft voting strategy.\n",
    "\n",
    "**3.2 Validation Task**\n",
    "\n",
    "The validation task evaluates model generalizability using 15% of the dataset held out during the training phase. Stratified sampling ensures class balance in validation data. Evaluation metrics such as accuracy, precision, recall, and F1-score are computed on the validation set. Additionally, 5-fold cross-validation is conducted during model training to mitigate overfitting and estimate performance stability.\n",
    "\n",
    "**3.3 Performance Definition**\n",
    "\n",
    "Model performance is defined by multiple metrics:\n",
    "- Accuracy: Measures the proportion of correctly classified samples in the test set.\n",
    "- Precision and Recall: Evaluate the model's ability to avoid false positives and negatives, particularly important in imbalanced datasets.\n",
    "- F1-score: Provides a harmonic mean of precision and recall, useful for overall evaluation.\n",
    "- Confusion Matrix: Offers a detailed view of classification outcomes per class, enabling the identification of specific error patterns.\n",
    "- For ensemble models, comparative analysis against individual models is conducted to highlight performance gains.\n",
    "\n",
    "**3.4 Data Preprocessing**\n",
    "\n",
    "- Extract multi-dimensional acoustic features from the raw audio, including MFCC, chroma features, and Mel spectrograms\n",
    "- Creatively perform data augmentation through methods such as adding sine wave noise, altering pitch, and changing speed\n",
    "- Apply feature standardization to ensure consistent scale across all features\n",
    "\n",
    "**3.5 Feature Engineering**\n",
    "\n",
    "- Extract comprehensive acoustic features:\n",
    "  * MFCC (40 coefficients): Capture vocal tract configuration\n",
    "  * Chroma features: Represent pitch and harmonic content\n",
    "  * Mel spectrograms: Encode frequency characteristics\n",
    "- Calculate statistical measures (mean and standard deviation) for each feature type\n",
    "- Standardize features using StandardScaler to normalize the feature distribution\n",
    "\n",
    "**3.6 Model Selection and Ensemble Learning**\n",
    "\n",
    "- Base Models Selection:\n",
    "  * SVM: Primary classifier optimized for high-dimensional feature spaces\n",
    "  * Random Forest: For capturing complex feature interactions\n",
    "  * KNN: To identify local patterns in the feature space\n",
    "- Ensemble Strategy:\n",
    "  * Implement soft voting mechanism\n",
    "  * Assign weighted importance to different classifiers\n",
    "  * Optimize model parameters through grid search and cross-validation\n",
    "\n",
    "**3.7 Model Evaluation**\n",
    "\n",
    "- Performance Metrics:\n",
    "  * Accuracy, precision, recall, and F1-score\n",
    "  * Cross-validation scores to assess model stability\n",
    "  * Individual and ensemble model performance comparison\n",
    "- Evaluation Strategy:\n",
    "  * 85/15 train-test split with stratification\n",
    "  * 5-fold cross-validation for robust performance estimation\n",
    "  * Comparative analysis of individual and ensemble models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1nDXnzYLLH6"
   },
   "source": [
    "# 4 Implemented ML Prediction Pipelines\n",
    "\n",
    "The implemented ML prediction pipelines are designed to process raw audio data and output predictions on whether a given audio file represents a \"Deceptive Story\" or a \"True Story\". Each pipeline comprises multiple stages, as outlined below:  \n",
    "- **Input**:  \n",
    "  * Raw audio files from the MLEnd Deception Dataset, pre-labeled as deceptive (0) or true (1).  \n",
    "\n",
    "- **Pipeline Stages**:  \n",
    "  * **Data Loading and Preprocessing**: Audio files are loaded, and key acoustic features (e.g., MFCC, chroma features, Mel spectrograms) are extracted. These features are standardized to ensure consistent scaling. Data augmentation techniques are applied to improve robustness.  \n",
    "  * **Feature Engineering**: Extracted features are summarized into statistical measures (e.g., mean, standard deviation) and transformed into fixed-length vectors for downstream processing.  \n",
    "  * **Model Training**: Three base classifiers (SVM, Random Forest, and KNN) are trained on the processed data using optimized hyperparameters identified through grid search.  \n",
    "  * **Ensemble Learning**: Predictions from the base classifiers are combined using a soft voting mechanism to produce a final output.  \n",
    "  * **Evaluation**: Validation and test sets are used to assess the model's performance through metrics such as accuracy, precision, recall, and F1-score.  \n",
    "\n",
    "- **Intermediate Data Structures**:  \n",
    "   * **Feature Matrices**: Generated during preprocessing, each row corresponds to an audio sample, and columns represent standardized features.  \n",
    "  * **Model Outputs**: Each classifier outputs probability distributions over the target classes, which are aggregated in the ensemble stage.  \n",
    "\n",
    "- **Output**:  \n",
    "  * Final predictions indicating whether an audio sample is deceptive or true, along with performance metrics to evaluate the pipeline's effectiveness.  \n",
    "\n",
    "<font color=\"blue\">\n",
    "Before running the following code, please place the dataset folder \"CBU0521DD_stories\" and the dataset attributes file \"CBU0521DD_stories_attributes.csv\" in the same directory as this .ipynb file.\n",
    "</font>\n",
    "\n",
    "**4.1 Transformation Stage**\n",
    "\n",
    "4.1.1 Data Loading and Reading：\n",
    "\n",
    "Import necessary libraries, set data paths, and read label data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:08:16.755099Z",
     "start_time": "2024-12-28T18:07:38.614066Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set data paths\n",
    "audio_path = \"CBU0521DD_stories\"\n",
    "csv_path = \"CBU0521DD_stories_attributes.csv\"\n",
    "\n",
    "# Read label data\n",
    "labels_df = pd.read_csv(csv_path)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2 Feature Extraction：\n",
    "\n",
    "Define functions to extract features from audio files, including MFCC, Chroma, and Mel Spectrogram features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:11:40.651071Z",
     "start_time": "2024-12-28T18:11:40.633003Z"
    }
   },
   "source": [
    "def extract_features(file_name):\n",
    "    y, sr = librosa.load(file_name, duration=300)\n",
    "    return extract_features_from_array(y, sr)\n",
    "\n",
    "def extract_features_from_array(y, sr):\n",
    "    # Extract MFCC mean and standard deviation\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    mfccs_std = np.std(mfccs, axis=1)\n",
    "    \n",
    "    # Extract Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "    chroma_std = np.std(chroma, axis=1)\n",
    "    \n",
    "    # Extract Mel Spectrogram features\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    mel_mean = np.mean(mel, axis=1)\n",
    "    mel_std = np.std(mel, axis=1)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.hstack([mfccs_mean, mfccs_std, chroma_mean, chroma_std, mel_mean, mel_std])\n",
    "    return features\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3 Data Augmentation Function:\n",
    "\n",
    "Enhance audio data by adding noise, changing pitch, and altering speed to increase data diversity and model robustness."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:11:42.794177Z",
     "start_time": "2024-12-28T18:11:42.787973Z"
    }
   },
   "source": [
    "def augment_audio(y, sr):\n",
    "    # Add fixed sine wave noise\n",
    "    t = np.arange(len(y))\n",
    "    noise = 0.005 * np.sin(2 * np.pi * t / 100)  # Fixed sine wave noise\n",
    "    y_noise = y + noise\n",
    "    \n",
    "    # Change pitch\n",
    "    y_pitch = librosa.effects.pitch_shift(y, n_steps=2, sr=sr)\n",
    "    \n",
    "    # Change speed\n",
    "    y_speed = librosa.effects.time_stretch(y, rate=0.9)\n",
    "    \n",
    "    return [y_noise, y_pitch, y_speed]\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.4 Data Preprocessing:\n",
    "\n",
    "Perform feature extraction, data augmentation, feature scaling, and split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:17:59.452641Z",
     "start_time": "2024-12-28T18:11:43.652649Z"
    }
   },
   "source": [
    "# Prepare dataset\n",
    "print(\"Starting to process original data...\")\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Process original data\n",
    "for index, row in labels_df.iterrows():\n",
    "    file_path = os.path.join(audio_path, row['filename'])\n",
    "    features.append(extract_features(file_path))\n",
    "    labels.append(row['Story_type'])\n",
    "    print(f'Processing original file: {row[\"filename\"]}')\n",
    "\n",
    "# Data augmentation\n",
    "print(\"\\nStarting data augmentation...\")\n",
    "for index, row in labels_df.iterrows():\n",
    "    file_path = os.path.join(audio_path, row['filename'])\n",
    "    y, sr = librosa.load(file_path, duration=300)\n",
    "    augmented_audios = augment_audio(y, sr)\n",
    "    for y_aug in augmented_audios:\n",
    "        features.append(extract_features_from_array(y_aug, sr))\n",
    "        labels.append(row['Story_type'])\n",
    "    print(f'Processing augmented data: {row[\"filename\"]}')\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Feature scaling\n",
    "print(\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "print(\"Splitting dataset...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process original data...\n",
      "Processing original file: 00001.wav\n",
      "Processing original file: 00002.wav\n",
      "Processing original file: 00003.wav\n",
      "Processing original file: 00004.wav\n",
      "Processing original file: 00005.wav\n",
      "Processing original file: 00006.wav\n",
      "Processing original file: 00007.wav\n",
      "Processing original file: 00008.wav\n",
      "Processing original file: 00009.wav\n",
      "Processing original file: 00010.wav\n",
      "Processing original file: 00011.wav\n",
      "Processing original file: 00012.wav\n",
      "Processing original file: 00013.wav\n",
      "Processing original file: 00014.wav\n",
      "Processing original file: 00015.wav\n",
      "Processing original file: 00016.wav\n",
      "Processing original file: 00017.wav\n",
      "Processing original file: 00018.wav\n",
      "Processing original file: 00019.wav\n",
      "Processing original file: 00020.wav\n",
      "Processing original file: 00021.wav\n",
      "Processing original file: 00022.wav\n",
      "Processing original file: 00023.wav\n",
      "Processing original file: 00024.wav\n",
      "Processing original file: 00025.wav\n",
      "Processing original file: 00026.wav\n",
      "Processing original file: 00027.wav\n",
      "Processing original file: 00028.wav\n",
      "Processing original file: 00029.wav\n",
      "Processing original file: 00030.wav\n",
      "Processing original file: 00031.wav\n",
      "Processing original file: 00032.wav\n",
      "Processing original file: 00033.wav\n",
      "Processing original file: 00034.wav\n",
      "Processing original file: 00035.wav\n",
      "Processing original file: 00036.wav\n",
      "Processing original file: 00037.wav\n",
      "Processing original file: 00038.wav\n",
      "Processing original file: 00039.wav\n",
      "Processing original file: 00040.wav\n",
      "Processing original file: 00041.wav\n",
      "Processing original file: 00042.wav\n",
      "Processing original file: 00043.wav\n",
      "Processing original file: 00044.wav\n",
      "Processing original file: 00045.wav\n",
      "Processing original file: 00046.wav\n",
      "Processing original file: 00047.wav\n",
      "Processing original file: 00048.wav\n",
      "Processing original file: 00049.wav\n",
      "Processing original file: 00050.wav\n",
      "Processing original file: 00051.wav\n",
      "Processing original file: 00052.wav\n",
      "Processing original file: 00053.wav\n",
      "Processing original file: 00054.wav\n",
      "Processing original file: 00055.wav\n",
      "Processing original file: 00056.wav\n",
      "Processing original file: 00057.wav\n",
      "Processing original file: 00058.wav\n",
      "Processing original file: 00059.wav\n",
      "Processing original file: 00060.wav\n",
      "Processing original file: 00061.wav\n",
      "Processing original file: 00062.wav\n",
      "Processing original file: 00063.wav\n",
      "Processing original file: 00064.wav\n",
      "Processing original file: 00065.wav\n",
      "Processing original file: 00066.wav\n",
      "Processing original file: 00067.wav\n",
      "Processing original file: 00068.wav\n",
      "Processing original file: 00069.wav\n",
      "Processing original file: 00070.wav\n",
      "Processing original file: 00071.wav\n",
      "Processing original file: 00072.wav\n",
      "Processing original file: 00073.wav\n",
      "Processing original file: 00074.wav\n",
      "Processing original file: 00075.wav\n",
      "Processing original file: 00076.wav\n",
      "Processing original file: 00077.wav\n",
      "Processing original file: 00078.wav\n",
      "Processing original file: 00079.wav\n",
      "Processing original file: 00080.wav\n",
      "Processing original file: 00081.wav\n",
      "Processing original file: 00082.wav\n",
      "Processing original file: 00083.wav\n",
      "Processing original file: 00084.wav\n",
      "Processing original file: 00085.wav\n",
      "Processing original file: 00086.wav\n",
      "Processing original file: 00087.wav\n",
      "Processing original file: 00088.wav\n",
      "Processing original file: 00089.wav\n",
      "Processing original file: 00090.wav\n",
      "Processing original file: 00091.wav\n",
      "Processing original file: 00092.wav\n",
      "Processing original file: 00093.wav\n",
      "Processing original file: 00094.wav\n",
      "Processing original file: 00095.wav\n",
      "Processing original file: 00096.wav\n",
      "Processing original file: 00097.wav\n",
      "Processing original file: 00098.wav\n",
      "Processing original file: 00099.wav\n",
      "Processing original file: 00100.wav\n",
      "\n",
      "Starting data augmentation...\n",
      "Processing augmented data: 00001.wav\n",
      "Processing augmented data: 00002.wav\n",
      "Processing augmented data: 00003.wav\n",
      "Processing augmented data: 00004.wav\n",
      "Processing augmented data: 00005.wav\n",
      "Processing augmented data: 00006.wav\n",
      "Processing augmented data: 00007.wav\n",
      "Processing augmented data: 00008.wav\n",
      "Processing augmented data: 00009.wav\n",
      "Processing augmented data: 00010.wav\n",
      "Processing augmented data: 00011.wav\n",
      "Processing augmented data: 00012.wav\n",
      "Processing augmented data: 00013.wav\n",
      "Processing augmented data: 00014.wav\n",
      "Processing augmented data: 00015.wav\n",
      "Processing augmented data: 00016.wav\n",
      "Processing augmented data: 00017.wav\n",
      "Processing augmented data: 00018.wav\n",
      "Processing augmented data: 00019.wav\n",
      "Processing augmented data: 00020.wav\n",
      "Processing augmented data: 00021.wav\n",
      "Processing augmented data: 00022.wav\n",
      "Processing augmented data: 00023.wav\n",
      "Processing augmented data: 00024.wav\n",
      "Processing augmented data: 00025.wav\n",
      "Processing augmented data: 00026.wav\n",
      "Processing augmented data: 00027.wav\n",
      "Processing augmented data: 00028.wav\n",
      "Processing augmented data: 00029.wav\n",
      "Processing augmented data: 00030.wav\n",
      "Processing augmented data: 00031.wav\n",
      "Processing augmented data: 00032.wav\n",
      "Processing augmented data: 00033.wav\n",
      "Processing augmented data: 00034.wav\n",
      "Processing augmented data: 00035.wav\n",
      "Processing augmented data: 00036.wav\n",
      "Processing augmented data: 00037.wav\n",
      "Processing augmented data: 00038.wav\n",
      "Processing augmented data: 00039.wav\n",
      "Processing augmented data: 00040.wav\n",
      "Processing augmented data: 00041.wav\n",
      "Processing augmented data: 00042.wav\n",
      "Processing augmented data: 00043.wav\n",
      "Processing augmented data: 00044.wav\n",
      "Processing augmented data: 00045.wav\n",
      "Processing augmented data: 00046.wav\n",
      "Processing augmented data: 00047.wav\n",
      "Processing augmented data: 00048.wav\n",
      "Processing augmented data: 00049.wav\n",
      "Processing augmented data: 00050.wav\n",
      "Processing augmented data: 00051.wav\n",
      "Processing augmented data: 00052.wav\n",
      "Processing augmented data: 00053.wav\n",
      "Processing augmented data: 00054.wav\n",
      "Processing augmented data: 00055.wav\n",
      "Processing augmented data: 00056.wav\n",
      "Processing augmented data: 00057.wav\n",
      "Processing augmented data: 00058.wav\n",
      "Processing augmented data: 00059.wav\n",
      "Processing augmented data: 00060.wav\n",
      "Processing augmented data: 00061.wav\n",
      "Processing augmented data: 00062.wav\n",
      "Processing augmented data: 00063.wav\n",
      "Processing augmented data: 00064.wav\n",
      "Processing augmented data: 00065.wav\n",
      "Processing augmented data: 00066.wav\n",
      "Processing augmented data: 00067.wav\n",
      "Processing augmented data: 00068.wav\n",
      "Processing augmented data: 00069.wav\n",
      "Processing augmented data: 00070.wav\n",
      "Processing augmented data: 00071.wav\n",
      "Processing augmented data: 00072.wav\n",
      "Processing augmented data: 00073.wav\n",
      "Processing augmented data: 00074.wav\n",
      "Processing augmented data: 00075.wav\n",
      "Processing augmented data: 00076.wav\n",
      "Processing augmented data: 00077.wav\n",
      "Processing augmented data: 00078.wav\n",
      "Processing augmented data: 00079.wav\n",
      "Processing augmented data: 00080.wav\n",
      "Processing augmented data: 00081.wav\n",
      "Processing augmented data: 00082.wav\n",
      "Processing augmented data: 00083.wav\n",
      "Processing augmented data: 00084.wav\n",
      "Processing augmented data: 00085.wav\n",
      "Processing augmented data: 00086.wav\n",
      "Processing augmented data: 00087.wav\n",
      "Processing augmented data: 00088.wav\n",
      "Processing augmented data: 00089.wav\n",
      "Processing augmented data: 00090.wav\n",
      "Processing augmented data: 00091.wav\n",
      "Processing augmented data: 00092.wav\n",
      "Processing augmented data: 00093.wav\n",
      "Processing augmented data: 00094.wav\n",
      "Processing augmented data: 00095.wav\n",
      "Processing augmented data: 00096.wav\n",
      "Processing augmented data: 00097.wav\n",
      "Processing augmented data: 00098.wav\n",
      "Processing augmented data: 00099.wav\n",
      "Processing augmented data: 00100.wav\n",
      "\n",
      "Scaling features...\n",
      "Splitting dataset...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F5_kI95LuZ2"
   },
   "source": [
    "**4.2 Model Stage**\n",
    "\n",
    "In this stage, we focus on developing and optimizing individual models before ensemble integration:\n",
    "\n",
    "1. **Support Vector Machine (SVM)**:\n",
    "   - Optimize parameters through grid search\n",
    "   - Key parameters explored: C, gamma, and kernel type\n",
    "   - Best configuration found through cross-validation\n",
    "\n",
    "2. **Model Parameter Optimization**:\n",
    "\n",
    "   - The process of conducting grid search and finding the best model parameters is shown as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:18:59.248469Z",
     "start_time": "2024-12-28T18:18:58.225433Z"
    }
   },
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the optimized parameters, we can use the optimized parameters to train the model. \n",
    "\n",
    "Due to differences in Python's and libraries' versions, the parameters obtained from grid search may be different, which can lead to differences in the accuracy of the final model. Hence, I will directly use the parameters that I have searched here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:19:00.450167Z",
     "start_time": "2024-12-28T18:19:00.377612Z"
    }
   },
   "source": [
    "# Train SVM model with optimal parameters\n",
    "# Set optimal parameters\n",
    "print(\"Training SVM model...\")\n",
    "svm = SVC(\n",
    "    C=10,\n",
    "    gamma='scale',\n",
    "    kernel='rbf',\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    "    max_iter=10000,\n",
    "    cache_size=2000,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "svm.fit(X_train, y_train)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=2000, class_weight='balanced', max_iter=10000,\n",
       "    probability=True, random_state=42)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, cache_size=2000, class_weight=&#x27;balanced&#x27;, max_iter=10000,\n",
       "    probability=True, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=10, cache_size=2000, class_weight=&#x27;balanced&#x27;, max_iter=10000,\n",
       "    probability=True, random_state=42)</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 Ensemble Stage**\n",
    "\n",
    "In this project, I employed a VotingClassifier to combine three base classifiers: Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbors (KNN). The rationale behind this ensemble approach is as follows:\n",
    "\n",
    "1. **Model Complementarity**:\n",
    "   - SVM: Excels in handling high-dimensional feature spaces and performs well in non-linear audio feature classification\n",
    "   - RF: Effectively manages complex feature interactions and provides feature importance analysis\n",
    "   - KNN: Captures local patterns in audio features based on similarity metrics\n",
    "\n",
    "2. **Ensemble Strategy**:\n",
    "   - Implements soft voting to incorporate probability predictions from all classifiers\n",
    "   - Assigns weights [2, 2, 1] to SVM, RF, and KNN respectively\n",
    "   - Weight distribution reflects each model's relative importance in the task\n",
    "\n",
    "3. **Model Configuration**:\n",
    "   - The following code configure and initialize each base classifier and set up the parameters for the voting classifier to implement ensemble learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T18:19:06.135549Z",
     "start_time": "2024-12-28T18:19:02.745892Z"
    }
   },
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize base classifiers\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',\n",
    "    algorithm='auto',\n",
    "    leaf_size=30,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('svm', svm), ('rf', rf), ('knn', knn)],\n",
    "    voting='soft',\n",
    "    weights=[2, 2, 1]\n",
    ")\n",
    "\n",
    "# Train Voting Classifier\n",
    "print(\"Training Voting Classifier...\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "# SVM\n",
    "print(\"\\n1. train SVM model...\")\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "print(\"SVM model accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"\\nSVM classification report:\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# RF\n",
    "print(\"\\n2. train RF model...\")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print(\"RF model accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"\\nRF classification report:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "\n",
    "# KNN\n",
    "print(\"\\n3. train KNN model...\")\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print(\"KNN model accuracy:\", accuracy_score(y_test, knn_pred))\n",
    "print(\"\\nKNN classification report:\")\n",
    "print(classification_report(y_test, knn_pred))\n",
    "\n",
    "print(\"\\nPredicting & Evaluating the ensemble mmodel...\")\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy * 100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Voting Classifier...\n",
      "\n",
      "1. train SVM model...\n",
      "SVM model accuracy: 0.9333333333333333\n",
      "\n",
      "SVM classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Deceptive Story       0.91      0.97      0.94        30\n",
      "     True Story       0.96      0.90      0.93        30\n",
      "\n",
      "       accuracy                           0.93        60\n",
      "      macro avg       0.94      0.93      0.93        60\n",
      "   weighted avg       0.94      0.93      0.93        60\n",
      "\n",
      "\n",
      "2. train RF model...\n",
      "RF model accuracy: 0.9333333333333333\n",
      "\n",
      "RF classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Deceptive Story       0.88      1.00      0.94        30\n",
      "     True Story       1.00      0.87      0.93        30\n",
      "\n",
      "       accuracy                           0.93        60\n",
      "      macro avg       0.94      0.93      0.93        60\n",
      "   weighted avg       0.94      0.93      0.93        60\n",
      "\n",
      "\n",
      "3. train KNN model...\n",
      "KNN model accuracy: 0.8166666666666667\n",
      "\n",
      "KNN classification report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Deceptive Story       0.79      0.87      0.83        30\n",
      "     True Story       0.85      0.77      0.81        30\n",
      "\n",
      "       accuracy                           0.82        60\n",
      "      macro avg       0.82      0.82      0.82        60\n",
      "   weighted avg       0.82      0.82      0.82        60\n",
      "\n",
      "\n",
      "Predicting & Evaluating the ensemble mmodel...\n",
      "Model Accuracy: 96.67%\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Deceptive Story       0.94      1.00      0.97        30\n",
      "     True Story       1.00      0.93      0.97        30\n",
      "\n",
      "       accuracy                           0.97        60\n",
      "      macro avg       0.97      0.97      0.97        60\n",
      "   weighted avg       0.97      0.97      0.97        60\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZQPxztuL9AW"
   },
   "source": [
    "# 5 Dataset\n",
    "\n",
    "Dataset Construction Process:\n",
    "\n",
    "1.  Raw Data:\n",
    "\n",
    "- Use audio files from `CBU0521DD_stories_attributes.csv`;\n",
    "- Total number of samples: 100 audio files;\n",
    "- Labels: \"Deceptive Story\" (0) and \"True Story\" (1);\n",
    "\n",
    "2.  Data Partitioning:\n",
    "\n",
    "- Training set: 85% (85 samples);\n",
    "- Test set: 15% (15 samples);\n",
    "\n",
    "3.  Data Augmentation:\n",
    "\n",
    "- Use fixed sine wave noise to enhance model stability;\n",
    "- Pitch shifting;\n",
    "- Speed variation;\n",
    "\n",
    "This increases the number of training samples to four times the original, totaling 360 audio files.\n",
    "\n",
    "4.  Feature Extraction:\n",
    "\n",
    "- MFCC features (mean and standard deviation);\n",
    "- Chroma features (mean and standard deviation);\n",
    "- Mel-spectrogram features (mean and standard deviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf7GN1aeXJI"
   },
   "source": [
    "# 6 Experiments and Results\n",
    "\n",
    "**6.1 Model Training Results**\n",
    "\n",
    "SVM Optimal Parameter Configuration:\n",
    "\n",
    "```\n",
    "{\n",
    "    'C': 10,\n",
    "    'gamma': 'scale',\n",
    "    'kernel': 'rbf'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2 Model Evaluation Results and Analysis**\n",
    "\n",
    "1. **Overall Performance**:\n",
    "- **SVM**: Accuracy: 93.33%\n",
    "- **Random Forest**: Accuracy: 93.33%\n",
    "- **KNN**: Accuracy: 81.67%\n",
    "- **Ensemble**: Accuracy: 96.67%\n",
    "\n",
    "2. **Classification Details (Ensemble)**:\n",
    "- Deceptive Story (0): Precision 94%, Recall 100%\n",
    "- True Story (1): Precision 100%, Recall 93%\n",
    "\n",
    "3. **Cross-validation**:\n",
    "- SVM: Average accuracy 84.12% (+/- 8.19%)\n",
    "- Random Forest: Average accuracy 81.76% (+/- 10.78%)\n",
    "- KNN: Average accuracy 76.47% (+/- 8.72%)\n",
    "- Ensemble: Average accuracy 84.12% (+/- 5.06%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final ensemble model, the F1 scores for both classes are 0.97 and 0.97 respectively, indicating balanced performance.\n",
    "\n",
    "Further parameter adjustments can be made to optimize the model's classification performance.\n",
    "\n",
    "```\n",
    "Model Accuracy: 96.67%\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      1.00      0.97        30\n",
    "           1       1.00      0.93      0.97        30\n",
    "\n",
    "    accuracy                           0.97        60\n",
    "   macro avg       0.97      0.97      0.97        60\n",
    "weighted avg       0.97      0.97      0.97        60\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSrJCR_cekPO"
   },
   "source": [
    "# 7 Conclusions\n",
    "\n",
    "This project successfully demonstrated the feasibility of using audio feature extraction and machine learning models, specifically Support Vector Machines (SVM), to predict the veracity of narrated stories.\n",
    "\n",
    "The ensemble approach combining SVM, RandomForest, and KNN further improved accuracy from 93.33% to 96.67%, demonstrating the complementary nature of these classifiers.\n",
    "Additionally, cross-validation results showed that the ensemble model effectively balances individual classifier weaknesses, underscoring the importance of model synergy.\n",
    "\n",
    "Key findings and achievements include:\n",
    "\n",
    "1. **Audio Feature Extraction**: The use of MFCC, chroma features, and Mel spectrograms proved effective in capturing the nuanced characteristics of audio signals relevant to classification tasks.\n",
    "2. **Model Performance**: The optimized SVM model achieved commendable performance, as evidenced by evaluation metrics like precision and recall. The ensemble approach further improved accuracy, demonstrating its complementary strengths.\n",
    "3. **Practical Implications**: This approach offers potential applications in fields such as fraud detection and psychological analysis.\n",
    "\n",
    "### Suggestions for Future Work:\n",
    "- **Dataset Expansion**: Enhancing the dataset with more diverse samples could improve model robustness and generalization.\n",
    "- **Real-time Implementation**: Developing a real-time system for audio analysis could make this solution more practical for real-world applications.\n",
    "- **Deep Learning Integration**: Explore the integration of deep learning models such as CNNs or RNNs after collecting larger datasets to capture temporal and spatial patterns in audio data.\n",
    "\n",
    "In summary, this project lays a strong foundation for further exploration and refinement in the field of audio-based truth prediction, offering both theoretical insights and practical potential. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 References\n",
    "\n",
    "[1] Oravec, Jo Ann. \"The emergence of “truth machines”?: Artificial intelligence approaches to lie detection.\" Ethics and Information Technology 24.1 (2022): 6.\n",
    "\n",
    "[2] Mohan, Karnati, and Ayan Seal. \"Deception detection on “Bag-of-Lies”: integration of multi-modal data using machine learning algorithms.\" Proceedings of International Conference on Machine Intelligence and Data Science Applications: MIDAS 2020. Springer Singapore, 2021.\n",
    "\n",
    "[3] Abdelwahab, Abdelrahman, et al. \"Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features.\" arXiv preprint arXiv:2411.08885 (2024)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
